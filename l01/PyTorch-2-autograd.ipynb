{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ai0st7YcGMtU"
   },
   "source": [
    "PyTorch: Autograd\n",
    "====\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Fww2IvzGMtX"
   },
   "source": [
    "**Autograd** is PyTorch' package for automatic differentiation for all operations on Tensors. It's a *define-by-run* framework - backpropagation is defined by how the code runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rKhQB9KAGMta"
   },
   "source": [
    "`.requires_grad = True`\n",
    "----\n",
    "\n",
    "This attribute sets the tensor to track all operations on it. After finishing computation you can then call `.backward()` to automatically compute all the gradients and store them into `.grad` attribute of each tensor.\n",
    "\n",
    "`.detach()` stops the tensor from tracking history, preventing future computation from being tracked.\n",
    "\n",
    "To stop tracking history in a block of code you can wrap it in `with torch.no_grad():`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VAg8t-f6GMtc"
   },
   "source": [
    "`Function`\n",
    "----\n",
    "\n",
    "Every operation performed on a Tensor creates a new `Function` object, that performs the computation and records that it happened. Alltogether they build up an acyclic graph, encoding a complete history of computation. Tensor's attribute `.grad_fn` refers to a `Function` used to create the Tensor (except for Tensors created by the user, where `.grad_fn is None`).\n",
    "\n",
    "If you want to compute the derivatives, you can call `.backward()` on a `Tensor`. If the `Tensor` is a scalar (holds one-element data) there is no need to pass any arguments to `.backward()`. If you are using a vector, you need to specify a `gradient` argument, which is a tensor of a matching shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kE2WfjyzGMte"
   },
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ABXYX2vGMtq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# create a tensor with requires_grad\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZKTyxIolGMtu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SubBackward0 object at 0x1154a8690>\n"
     ]
    }
   ],
   "source": [
    "# perform a simple operation and check the `grad_fn`\n",
    "y = x - 4\n",
    "print(y.grad_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vb74wl6zGMt0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[45., 45.],\n",
      "        [45., 45.]], grad_fn=<MulBackward0>)\n",
      "tensor(45., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# perform some more operations\n",
    "z = y * y * 5\n",
    "out = z.mean()\n",
    "\n",
    "print(z)  # see the grad_fn\n",
    "print(out)  # see the grad_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uz98X1M5GMt4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "None\n",
      "----\n",
      "True\n",
      "tensor(1179.2075, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# as earlier, `.requires_grad_(...) changes the flag in-place\n",
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)  # <- this will be None\n",
    "print('----')\n",
    "\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wRoTJPeXGMt8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-7.5000, -7.5000],\n",
      "        [-7.5000, -7.5000]])\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# perform backpropagation on `out` and calculate the gradients\n",
    "# since `out` contains a single scalar, there is no need to pass arguments\n",
    "out.backward()\n",
    "# equivalent: out.backward(torch.tensor(1.))\n",
    "print(x.grad)  # d(out)/dx\n",
    "\n",
    "# .backward() accumulates gradient only in the leaf nodes\n",
    "# that is why for y, z the grad is None\n",
    "print(y.grad)\n",
    "print(z.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HYG-G-IxGMt-"
   },
   "source": [
    "# Small mathematical note\n",
    "\n",
    "Let `out` be called $o$.\n",
    "\n",
    "$$o = \\frac{1}{4} \\sum_i{z_i}$$\n",
    "$$z_i = 5*\\left(x_i - 4\\right)^2$$\n",
    "$$z_i\\mid_{x_i=1} = 27$$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$\\frac{\\partial o}{\\partial x_i} = \\frac{5}{2} \\left(x_i - 4\\right)$$\n",
    "$$\\frac{\\partial o}{\\partial x_i}\\mid_{x_i=1} = -\\frac{15}{2} = -7.5$$\n",
    "\n",
    "## Gradient\n",
    "\n",
    "For a vector valued function $\\vec{y}=f\\left(\\vec{x}\\right)$, the gradient of $\\vec{y}$ with respect to $\\vec{x}$ is a Jacobian matrix:\n",
    "\n",
    "$$J = \\left(\\begin{array}{ccc}\\frac{\\partial y_1}{\\partial x_1} & \\dots & \\frac{\\partial y_1}{\\partial x_n}\\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial y_m}{\\partial x_1} & \\dots & \\frac{\\partial y_m}{\\partial x_n} \\end{array}\\right)$$\n",
    "\n",
    "`torch.autograd` is an engine for computing vector-Jacobian product - given any vector $v = \\left(\\begin{array}{cccc}v_1 & v_2 & \\dots & v_m\\end{array}\\right)^T$ compute a product $v^T \\cdot J$. If $v$ is a gradient of a scalar function $l=g\\left(\\vec{y}\\right)$ (that is $v = \\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_1}& \\dots & \\frac{\\partial l}{\\partial y_m}\\end{array}\\right)^T$), then, by chain rule, the vector-Jacobian product would be the gradient of $l$ with respect to $\\vec{x}$:\n",
    "\n",
    "$$J^T \\cdot v = \\left(\\begin{array}{ccc}\\frac{\\partial y_1}{\\partial x_1} & \\dots & \\frac{\\partial y_m}{\\partial x_1}\\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial y_1}{\\partial x_n} & \\dots & \\frac{\\partial y_m}{\\partial x_n} \\end{array}\\right) \\left(\\begin{array}{c}\\frac{\\partial l}{\\partial y_1} \\\\ \\vdots \\\\ \\frac{\\partial l}{\\partial y_m}\\end{array}\\right)=\\left(\\begin{array}{c}\\frac{\\partial l}{\\partial x_1} \\\\ \\vdots \\\\ \\frac{\\partial l}{\\partial x_n}\\end{array}\\right)$$\n",
    "\n",
    "> Note: $v^T \\cdot J$ gives a row vector which can be treated as a column vector by taking $J^T \\cdot v$.\n",
    "\n",
    "This characteristic of vector-Jacobian product makes it very convenient to feed external gradients into a model that has non-scalar output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nOucN8kMGMt_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 norm: 2.4482009410858154\n",
      "L2 norm: 4.896401882171631\n",
      "L2 norm: 9.792803764343262\n",
      "L2 norm: 19.585607528686523\n",
      "L2 norm: 39.17121505737305\n",
      "L2 norm: 78.3424301147461\n",
      "L2 norm: 156.6848602294922\n",
      "L2 norm: 313.3697204589844\n",
      "L2 norm: 626.7394409179688\n",
      "tensor([1091.7732, -185.3734,  587.2626], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# example of vector-Jacobian product\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000: # L2 norm\n",
    "    print(f'L2 norm: {y.data.norm()}')\n",
    "    y = y * 2\n",
    "\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "flXYX_uLGMuC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0240e+02, 1.0240e+03, 1.0240e-02])\n"
     ]
    }
   ],
   "source": [
    "# y is not a scalar - cannot calculate the Jacobian directly\n",
    "# you need to pass a vector as an argument - 3 element\n",
    "v = torch.tensor([0.1, 1.0, 0.00001], dtype=torch.float)  # e.g. from a loss function\n",
    "y.backward(v)\n",
    "\n",
    "print(x.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zyHC8j33GMuF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "-----\n",
      "False\n",
      "-----\n",
      "False\n",
      "tensor([True, True, True])\n"
     ]
    }
   ],
   "source": [
    "# stop autograd from tracking history\n",
    "print(x.requires_grad)\n",
    "print((2 * x).requires_grad)\n",
    "print('-----')\n",
    "\n",
    "# torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    print((2 * x).requires_grad)\n",
    "print('-----')\n",
    "    \n",
    "# .detach()\n",
    "detached = x.detach()\n",
    "print(detached.requires_grad)\n",
    "print((detached == x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fzp4DvndGMuG"
   },
   "source": [
    "Exercises\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wGFdkz_rGMuH"
   },
   "source": [
    "1. Create 3 torch Tensors (scalars): $x = 1$, $w = 0.27$ and $b = 3$, so that they will be tracking gradients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hYzjT25hGMuI"
   },
   "outputs": [],
   "source": [
    "x = torch.tensor(1., requires_grad=True)\n",
    "w = torch.tensor(0.27, requires_grad=True)\n",
    "b = torch.tensor(3., requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U7lY2Sg8GMuK"
   },
   "source": [
    "2. Calculate the following equation:\n",
    "\n",
    "$$y = w \\cdot x + b$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1519,
     "status": "ok",
     "timestamp": 1583486021663,
     "user": {
      "displayName": "Max Telepchuk",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GibTf7NVyvLjEqhcqQV-YR1cObff-CHAK7XQgn1vw=s64",
      "userId": "00289811963611034862"
     },
     "user_tz": -60
    },
    "id": "J0E66nLwGMuL",
    "outputId": "5e1a571b-e12f-4dd4-b25a-4fba148e7785"
   },
   "outputs": [],
   "source": [
    "y = w * x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "20yTMWQSGMuN"
   },
   "source": [
    "3. Compute and display the gradients for each value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WZ-XwhvCGMuO"
   },
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 456,
     "status": "ok",
     "timestamp": 1583486602783,
     "user": {
      "displayName": "Max Telepchuk",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GibTf7NVyvLjEqhcqQV-YR1cObff-CHAK7XQgn1vw=s64",
      "userId": "00289811963611034862"
     },
     "user_tz": -60
    },
    "id": "2qbkqThOIdmB",
    "outputId": "22a725b1-610b-4f02-eb2e-75e30f43ec29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2700)\n",
      "tensor(1.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)\n",
    "print(b.grad)\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3zIueGSBGMuQ"
   },
   "source": [
    "4. Calculate the result of another equation, compute gradients and display them.\n",
    "\n",
    "$$z = w \\cdot \\left(x ^ 2 - b\\right)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LzWcfCIvGMuR"
   },
   "outputs": [],
   "source": [
    "z = w*(x**2 - b)\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 859,
     "status": "ok",
     "timestamp": 1583486653794,
     "user": {
      "displayName": "Max Telepchuk",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GibTf7NVyvLjEqhcqQV-YR1cObff-CHAK7XQgn1vw=s64",
      "userId": "00289811963611034862"
     },
     "user_tz": -60
    },
    "id": "eq6ypsBrKHx1",
    "outputId": "9857af97-5db7-44ab-dc10-d8dbcae72643"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8100)\n",
      "tensor(0.7300)\n",
      "tensor(-1.)\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)\n",
    "print(b.grad)\n",
    "print(w.grad)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "PyTorch-2-autograd.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}