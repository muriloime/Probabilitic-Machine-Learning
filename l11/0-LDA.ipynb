{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook relies heavily on [Kevin Murphy's _Machine Learning A Probabilistic Perspective_ (p. 950)](https://www.cs.ubc.ca/~murphyk/MLbook/). The original work is described in [Latent Dirichlet Allocation](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf) by Blei et al. [Griffiths and Steyvers in _Finding scientific topics_](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC387300/pdf/1015228.pdf) extended LDA toolkit by proposing a new parameter estimation technique through the collapsed Gibbs sampling. Some parts are directly taken from these works. Also, you can find [_Latent Dirichlet Allocation: Towards a Deeper\n",
    "Understanding_](http://obphio.us/pdfs/lda_tutorial.pdf) helpful during the implementation.\n",
    "\n",
    "LDA belongs to the family of probabilistic graphical models that found many applications in working discrete data such gene descriptions and text corpora. This three-level hierarchical Bayesian model models each item in the collection with a finite mixture over an underlying set of topics. Each topic is modeled with a mixture of topic probabilities. These topic are not necessarily thematical topics in texts but in corpora they have a direct intrepretation. It is depicted as below\n",
    "\n",
    "![LDA](assets/lda.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does it work\n",
    "\n",
    "We assume that we work with text documents, hence it is easier to introduce a few concepts. The basic idea is that documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words. LDA assumes the following generative process for each document $\\mathbf{w}$ in a corpus $\\mathcal{D}$:\n",
    "\n",
    "1. Choose $N \\sim \\text{Poisson}(\\xi)$ (length of the document)\n",
    "2. Choose $\\theta \\sim \\text{Dir}(\\alpha)$ (choose probabilities of topics from the Dirichlet simplex, which conjugate to the Multinomial)\n",
    "3. For each of the $N$ words $w_n$:\n",
    "   1. Choose a topic $z_n \\sim \\text{Multinomial}(\\theta)$\n",
    "   2. Choose a word $w_n$ from $p(w_n | z_n, \\beta)$, a multinomial probability conditioned on the topic $z_n$ (each word has an assigned topic to it, $\\beta$ is a parameter of the prior over topics for words)\n",
    "   \n",
    "A few notes. $z_n$ is a $k$-dimensional simplex of topics. The $k$ is set upfront (however, we note that there exist methods for estimating it, such as Dirichlet Process) and stands for the number of topics. Word probabilities are parametrized by a $k \\times V$ matrix $\\beta$ ($V$ is a size of the vocabulary that has to be set upfront as well), and $\\beta_{ij} = p(w^j = 1 | z^i = 1)$ (probability of the $j$-th word occurence in the $i$-th topic context).\n",
    "\n",
    "Given the parameters $\\alpha$  and $\\beta$ (which have to be estimated during preprocessing of the corpus), the joint distribution of a topic mixture $\\theta$, a set of $N$ topics $\\mathbf{z}$, and a set of $N$ words $\\mathbf{w}$ is given by:\n",
    "\n",
    "$$\n",
    "p(\\boldsymbol{\\theta}, \\mathbf{z}, \\mathbf{w} | \\boldsymbol{\\alpha}, \\boldsymbol{\\beta}) = p(\\boldsymbol{\\theta} | \\boldsymbol{\\alpha}) \\prod^N_{n=1} p(z_n | \\boldsymbol{\\theta}) p(w_n | z_n, \\boldsymbol{\\beta})\n",
    "$$\n",
    "\n",
    "By marginilizing over parameters, we obtain the distribution of a single documents (how likely is a set of words for a particular document):\n",
    "\n",
    "$$\n",
    "p(\\mathbf{w} | \\boldsymbol{\\alpha}, \\boldsymbol{\\beta}) = \\int p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha})\\left(\\prod^N_{n=1} \\sum_{z_n} p(z_n | \\boldsymbol{\\theta}) p(w_n | z_n, \\boldsymbol{\\beta}) \\right)d\\boldsymbol{\\theta}\n",
    "$$\n",
    "\n",
    "and by taking the product of the marginal probabilities of single documents  - probability of a corpus:\n",
    "\n",
    "$$\n",
    "p(\\mathcal{D}|\\boldsymbol{\\alpha},\\boldsymbol{\\beta}) = \\prod^M_{d=1}\\int p(\\theta_d|\\boldsymbol{\\alpha}) \\left( \\prod^{N_d}_{n=1}\\sum_{z_{dn}}p(z_{dn} | \\theta_d)p(w_{dn} | z_{dn}, \\boldsymbol{\\beta}) \\right)d\\theta_d\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometrical interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LDA can considered as a dimensionality reduction algorithm (similar to the PCA). However, while the PCA works in Euclidean spaces, the LDA works in simplex spaces that, and thus it can handle ambiguities in words (so called \"polysemy\"). \n",
    "\n",
    "Words are distributed over $V - 1$ dimensional simplex (word simplex). The LDA considers $k$ points on the on the word simplex and forms a subsimplex of a lower dimensionality (topic simplex). Therefore, each word is cast on the subsimplex.\n",
    "\n",
    "LDA posits that each word of both the observed and unseen documents is generated by a randomly chosen topic which is drawn from a distribution with a randomly chosen parameter. This parameter is sampled once per document from a smooth distribution on the topic simplex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to complex variable coupling, visible in the plate model, it is intractable to derive exact inference. However, it is possible to perform some sort of decoupling by removing edges from the original model. The variational distribution is derived then as\n",
    "\n",
    "![Variational model](assets/variational.png)\n",
    "\n",
    "The distribution is described as\n",
    "\n",
    "$$\n",
    "q(\\boldsymbol{\\theta}, \\mathbf{z} | \\boldsymbol{\\gamma}, \\boldsymbol{\\phi}) = q(\\boldsymbol{\\theta}|\\boldsymbol{\\gamma}) \\prod^N_{n=1} q(z_n | \\phi_n)\n",
    "$$\n",
    "\n",
    "where the Dirichlet parameter $\\boldsymbol{\\gamma}$ and the multinomial parameters $(\\phi_1,...,\\phi_N)$ are the free variational parameters.  In the language of text, the optimizing parameters $(\\boldsymbol{\\gamma}(\\mathbf{w}),\\boldsymbol{\\phi}(\\mathbf{w}))$ are document-specific. In particular, we view the Dirichlet parameters $\\boldsymbol{\\gamma}(\\mathbf{w})$ as providing a representation of a document in the topic simplex.\n",
    "\n",
    "\n",
    "By setting KL divergence in the VI framework, we obtain update equations:\n",
    "\n",
    "\\begin{align}\n",
    "\\phi_{ni} &\\propto \\beta_{iw_n} \\exp\\{\\mathbb{E}_q[\\log(\\theta_i)|\\boldsymbol{\\gamma}]\\} \\\\\n",
    "\\gamma_i &= \\alpha_i + \\sum^N_{n=1} \\phi_{ni}\n",
    "\\end{align}\n",
    "\n",
    "The expectation is equal to the: \n",
    "\n",
    "$$\n",
    "\\mathbb{E}_q[\\log(\\theta_i)|\\boldsymbol{\\gamma}] = \\Psi(\\gamma_i) - \\Psi\\left(\\sum^k_{j=1} \\gamma_j\\right)\n",
    "$$\n",
    "\n",
    "where $\\Psi$ is [digamma function](https://en.wikipedia.org/wiki/Digamma_function) (implemented in `pytorch`)\n",
    "\n",
    "The algorithm for the inference is performed as alternated updates of these parameters through $t \\in \\{1, ..., T\\}$ steps.\n",
    "\n",
    "![Inference](assets/inference.png)\n",
    "\n",
    "We recall, that $k$ denotes number of topics (hyperparameter) and $N$ - number of words in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter estimation using Variational Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, given a corpus of documents $\\mathcal{D} = \\{\\mathbf{w}_1, ..., \\mathbf{w}_M\\}$ (M documents), we wish to find parameters $\\boldsymbol{\\alpha}$ and $\\boldsymbol{\\beta}$ that maximize the (marginal) log likelihood of the data:\n",
    "\n",
    "$$\n",
    "\\mathcal{l}(\\boldsymbol{\\alpha}, \\boldsymbol{\\beta}) = \\sum^M_{d=1} \\log p(\\mathbf{w}_d | \\boldsymbol{\\alpha}, \\boldsymbol{\\beta})\n",
    "$$\n",
    "\n",
    "The equation can be optimized through variational EM that maximizes a lower bound with respect to the variational parameters $\\boldsymbol{\\gamma}$ and $\\boldsymbol{\\phi}$, and, then for fixed values of the variational parameters, maximizes the lower bound with respect to the model parameters $\\boldsymbol{\\alpha}$ and $\\boldsymbol{\\beta}$. \n",
    "\n",
    "The algorithm is as follows:\n",
    "1. (E-step) For each document, find the optimizing values of the variational parameters $\\{\\boldsymbol{\\gamma}_d, \\boldsymbol{\\phi}_d : d \\in \\mathcal{D}\\}$. This is done as described in the previous section.\n",
    "\n",
    "2. (M-step) Maximize the resulting lower bound on the log likelihood with respect to the model parameters $\\boldsymbol{\\alpha}$ and $\\boldsymbol{\\beta}$. This corresponds to finding maximum likelihood estimates with expected sufficient statistics for each document under the approximate posterior which is computed in the E-step.\n",
    "\n",
    "These two steps are repeated until the lower bound on the log likelihood converges.\n",
    "\n",
    "The M-step can be written analitacilly as:\n",
    "\n",
    "$$\n",
    "\\beta_{ij} \\propto \\sum^M_{d=1}\\sum^{N_d}_{n=1} \\phi_{dni}w^j_{dn}\n",
    "$$\n",
    "\n",
    "Note that, $\\boldsymbol{\\beta}$ has to be normalized afterwards.\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_\\alpha = \\sum^M_{d=1} \n",
    "\\left( \n",
    "    \\log\\Gamma\n",
    "    \\left(\n",
    "        \\sum^k_{j=1}\\alpha_j\n",
    "    \\right) \n",
    "    - \\sum^k_{i=1} \\log\\Gamma\n",
    "    \\left(\n",
    "        \\alpha_i\n",
    "    \\right)\n",
    "    + \\sum^k_{i=1}\n",
    "    \\left(\n",
    "        \\left(\n",
    "            \\alpha_i -1\n",
    "        \\right)\n",
    "        \\left(\n",
    "            \\Psi\n",
    "            \\left(\n",
    "                \\gamma_{di}\n",
    "            \\right) \n",
    "            - \\Psi\n",
    "            \\left(\n",
    "                \\sum^k_{j=1}\\gamma_{dj}\n",
    "            \\right)\n",
    "        \\right) \n",
    "    \\right)\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "We have already implemented optimizing $\\alpha$ so you can focus on other implementation details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full LDA models consider one additional parameters can be estimated: $\\eta$. \n",
    "\n",
    "$\\eta$ is a parameter telling about smoothing. The smoothing is necessary due to long-tail distribution of words, where many words in the vocabulary can be absent in many documents. The common approach is to use Laplace smoothing (add some constant to each count entry), however it is not applicable for the LDA directly. Blei et al. defined a new node in the PGM of the LDA that holds additional smoothing values, that contribute to the join probability. Again, derivation of its values require application of the Newton-Raphson method (in a similar way). The data, you will use, was preprocessed in such a way, that it will not be necessary to estimate and use $\\eta$ params."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter estimation using Gibbs Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources:\n",
    "- _[Latent Dirichlet Allocation](http://acsweb.ucsd.edu/~yuw176/report/lda.pdf)_ (description of the perplexity calculation)\n",
    "- _[Finding scientific topics](https://www.pnas.org/content/pnas/101/suppl_1/5228.full.pdf)_ (first proposition of the collapsed Gibbs sampling for the LDA\n",
    "- _[Machine Learning: A Probabilistic Perspective](https://bit.ly/3cSB6g1)_ (p. 955)\n",
    "- _[A Theoretical and Practical Implementation Tutorial on Topic Modeling and Gibbs Sampling](http://www.ccs.neu.edu/home/vip/teach/DMcourse/5_topicmodel_summ/notes_slides/sampling/darling-lda.pdf)_\n",
    "\n",
    "The Gibbs Sampling approach corresponds to the \"fuller\" plate representation of the LDA:\n",
    "\n",
    "![fuller lda](https://www.researchgate.net/publication/303203779/figure/fig1/AS:485818495180800@1492839563094/Graphical-representation-of-the-LDA-model-The-shaded-circle-represents-the-observed.png)\n",
    "\n",
    "Gibbs Sampling is one member of a family of algorithms from the Markov Chain Monte Carlo (MCMC) framework. The MCMC algorithms aim to construct a Markov chain that has the target posterior distribution as its stationary distribution. In other words, after a number of iterations of stepping through the chain, sampling from the distribution should converge to be close to sampling.\n",
    "\n",
    "For LDA, we are interested in the latent document-topic portions $\\theta_d$ for the $d\\in \\mathcal{D}$ document, the topic-word distributions $\\phi$ of size $K \\times V$ (distribution of words per each $k$-th topic), and the topic index assignments $\\mathbf{z}_d$ for each word in $\\mathbf{w}_{d}$. While conditional distributions – and therefore an LDA Gibbs Sampling algorithm – can be derived for each of these latent variables, we note that both $\\theta_d$ and $\\phi$ can be calculated using just the topic index assignments $\\mathbf{z}_d$ (i.e. $\\mathbf{z}$ is a sufficient statistic for both these distributions). Therefore, a simpler algorithm can be used if we integrate out the multinomial parameters and simply sample $\\mathbf{z}_d$. This is called a collapsed Gibbs sampler.\n",
    "\n",
    "The collapsed Gibbs sampler for LDA needs to compute the probability of a topic z being assigned to a word $w_{d,i}$, given all other topic assignments to all other words. Somewhat more formally, we are interested in computing the following posterior up to a constant:\n",
    "\n",
    "$$\n",
    "p(z_{d,i} | \\mathbf{z}_{d,-i}, \\alpha, \\beta, \\mathbf{w}_d)\n",
    "$$\n",
    "\n",
    "where $\\mathbf{z}_{d,-i}$ means all topic allocations _except_ for $z_{d,i}$.\n",
    "\n",
    "Implementing an LDA collapsed Gibbs sampler is surprisingly straightforward. It involves setting up the requisite count variables, randomly initializing them, and then running a loop over the desired number of iterations where on each loop a topic is sampled for each word instance in the corpus. Following the Gibbs iterations, the counts can be used to compute the latent distributions $\\boldsymbol{\\theta}$ and $\\boldsymbol{\\phi}$. The only required count variables include $n_{d,k}$, the number of words assigned to topic $k$ in document $d$; and $n_{k,w}$, the number of times word $w$ is assigned to topic $k$. Finally, in addition to the obvious variables such as a representation of the corpus ($\\mathbf{w}$), we need an array $\\mathbf{z}$ which will contain the current topic assignment for each of the $N$ words in the corpus.\n",
    "\n",
    "The algorithm is defined as (notation is adapted from Murphy's book, refer there for more info):\n",
    "\n",
    "\n",
    "1. For each word $w_{d,i}$ sample a topic from\n",
    "\n",
    "$$\n",
    "z_{d,i} \\sim \\text{Categorical}(p(z_{d,i})), \\qquad  p(z_{d,i} = k | \\cdot) \\propto \\exp\\left\\{ \\log(\\theta_{d,k}) + \\log(\\phi_{k, v_{w_{d,i}}}) \\right\\}\n",
    "$$\n",
    "\n",
    "where $v_{w_{d,i}}$ denotes dictionary index of the word, $v_{w_{d,i}} \\in V$\n",
    "\n",
    "2. Estimate parameters of the topic $k$ distribution for each document $d$, $\\theta_{d,k}$\n",
    "\n",
    "        \n",
    "$$\n",
    "\\theta_{d,k}  = \\frac{\\alpha + n_{d,k}}{K\\alpha + n_d} \\qquad \\text{or} \\qquad p(\\theta_{d,k} | \\cdot) = \\text{Dirichlet}\\left(\\left\\{ \\alpha + \\sum_j\\mathbb{1}[z_{d,j} = k] \\right\\}\\right)\n",
    "$$\n",
    "\n",
    "where $n_{d,k}$ is a number of words in document $d$ that were assigned to the topic $k$, $n_d$ is a number of words in the $d$ document, and $\\mathbb{1}[z_{d,j} = k]$ returns one if the topic index for the word $w_{d,j}$ is equal to $k$. Both equations are equal, while the first one being the MAP estimate. It's your choice which one to implement, however both should converge to a similar solution.\n",
    "\n",
    "3. Estimate parameters of the word distribution for each topic $\\phi_{k,v}$\n",
    "        \n",
    "$$\n",
    "\\phi_{k, v} = \\frac{\\beta + n_{k, v}}{V\\beta + n_k} \\qquad \\text{or} \\qquad p(\\phi_{k,v} | \\cdot) = \\text{Dirichlet}\\left(\\left\\{\\beta + \\sum_{d\\in\\mathcal{D}}\\sum_{i} \\mathbb{1}[w_{d,i} = v, z_{d,i} = k] \\right\\}\\right)\n",
    "$$\n",
    "\n",
    "where $n_{k,v}$ is a number of assignments of word $v$ to the topic $k$ in all documents, $n_k$ is a number of all words assigned to the topic $k$, and $\\mathbb{1}[w_{d,i} = v, z_{d,i} = k]$ returns one if the word $w_{d,i}$ is the same word as $v$ and it is assigned to the topic $k$. Both equations are equal, while the first one being the MAP estimate. It's your choice which one to implement, however both should converge to a similar solution.\n",
    "\n",
    "4. Repeat until convergence or for predefined number of steps.\n",
    "\n",
    "The algorithm differs from the VG in such a way, that it does not learn / estimate $\\beta$ and $\\alpha$ parameters - these only serve as a prior knowledge about our model and are set as scalars in that frameworks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can evaluated using perplexity measure, a standard metric in natural language processing\n",
    "\n",
    "$$\n",
    "perplexity(\\mathcal{D}) = \\exp\\left\\{-\\frac{\\sum^M_{d=1} \\log p(\\mathbf{w}_d)}{\\sum^M_{d=1} N_d} \\right\\}\n",
    "$$\n",
    "\n",
    "in the general form, where\n",
    "\n",
    "$$ \n",
    "\\log p(\\mathbf{w}_d) = \\sum_{v \\in V} n_{d,v} \\log\\left( \\sum_{k\\in K}\\phi_{k,v}\\cdot \\theta_{d,k}  \\right)\n",
    "$$\n",
    "\n",
    "and $n_{d,v}$ denotes number of occurences of word $v$ in the $d$ document.\n",
    "\n",
    "\n",
    "Intuitively, perplexity measures the weighted average **branching factor** of the model's predictive distribution. Suppose the model predicts that each symbol (letter, word, whatever) is equally likely, so $p(y_i|\\mathbf{y}_{1:i-1}) = 1 / K$. Then the perplexity is $((1 / K)^N)^{-1/N} = N$. So for $V = 1000$, maximum perplexity is 1000, and mimnimal is 0. If some symbols are omre likely than others, and the model correctly reflects this, its perplexity will be lower than K. Of course, $H(p, p) = H(p) \\leq H(p,q)$ (for two stochastic processes $p$, $q$), so we can never reduce the perplexity below the entropy of the true underlying stochastic proceess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implement generating topic assignments for words\n",
    "- Implement estimation of topic distribution parameters for each document\n",
    "- Implement estimation of words distribution parameters for each topic\n",
    "- Implement calculating perplexity \n",
    "- Implement getting topic distribution for a word and most probable topic for a document (we asssume that these methods are evaluated on the train dataset only)\n",
    "\n",
    "**Note:** You can freely change parameters defined in `src/common.py` for debugging purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set I used is the 20Newsgroup data set. The data set is already preprocessed and both train and test splits consists of list of documents where each document $d$ has $N_d$ words. Words have been already lemmatized, stemmed and stop words have been removed. There are 20 targets in the data set — `alt.atheism`, `comp.graphics`, `comp.os.ms-windows.misc`, `comp.sys.ibm.pc.hardware`, `comp.sys.mac.hardware`, `comp.windows.x`, `misc.forsale`, `rec.autos`, `rec.motorcycles`, `rec.sport.baseball`, `rec.sport.hockey`, `sci.crypt`, `sci.electronics`, `sci.med`, `sci.space`, `soc.religion.christian`, `talk.politics.guns`, `talk.politics.mideast`, `talk.politics.misc`, `talk.religion.misc`.\n",
    "\n",
    "Looking visually we can say that this data set has a few broad topics like:\n",
    "- Science\n",
    "- Politics\n",
    "- Sports\n",
    "- Religion\n",
    "- Technology etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/telepchuk/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from typing import List\n",
    "\n",
    "import gensim\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributions as dist\n",
    "\n",
    "from src.base import BaseLDA\n",
    "from src.data_utils import get_newsgroup_dataset\n",
    "from src.visualization import sentences_chart, visualize_history, display_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_texts, test_texts), dictionary = get_newsgroup_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x129f26dd8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.kdeplot([len(doc) for doc in train_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x12c15e6d8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deXxc5Xno8d+jGWm0L5ZkS5ZkS94tDNjgmC0BElIwNI1JConJbUtbemla6JJ0g9tb2nJLW+4nTVIaml4aQkmaxlCatk4KoW3YSTCWsQFvsmXJ2izZo31fRnruH3PkCEXLYI3mnJl5vp+PPpx55z1HzzkW88y7nPeIqmKMMSb5pLgdgDHGGHdYAjDGmCRlCcAYY5KUJQBjjElSlgCMMSZJ+d0O4P0oKirSyspKt8Mwxpi4cuDAgQ5VLZ5ZHlcJoLKykpqaGrfDMMaYuCIijbOVWxeQMcYkKUsAxhiTpCwBGGNMkrIEYIwxScoSgDHGJClLAMYYk6QsARhjTJKyBJBgbHlvY0yk4upGMDO/p/Y38b//7TCbS3PZvnoZ120s5roNP3HznzHGANYCSBg9Q2P8xXPHqSrKIiPVx7f2NXLn19/klRNBt0MzxniUtQASxJf/+yR9w+PsuftKNpXkMjI+wYe/8BKP/OAkH1pfhIi4HaIxxmOsBZAATp7t55tvNPKZK1axqSQXgPRUH5+9bi01jd38qL7T5QiNMV5kCSDOqSoPfu8oWWk+Pv9TG9/z3qc/UEFxToCvvFDnUnTGGC+zBBDnXqoN8urJDj73UxtYlpX2nvfSU3386rVr+OGpTg40drkUoTHGqywBxLl/PdhKUXaAn7ty9azvf+aKVSzLSuORH1grwBjzXpYA4tjkpPJaXQfXbigi1Tf7P2Vmmp9f+VAVL58Icry9L8YRGmO8zBJAHDva1kfX4BjXrp9/rv/tl1cA8INj52IRljEmTlgCiGOvnAzP8b9mXdG89YpzAmwpy+XlWrsnwBjzYxElABHZKSK1IlInIvfN8n5ARJ5y3t8nIpVOeaGIvCgiAyLylTmOvVdEDi/mJJLVKyeCVJfmUpwTWLDuteuLeaupm76R8RhEZoyJBwsmABHxAY8CNwPVwB0iUj2j2l1At6quA74EPOyUjwB/BPzuHMf+JDBwYaEnt8HREAcau/nQhvm//U+5bkMxoUnlh3V2T4AxJiySFsAOoE5V61V1DNgD7JpRZxfwpLP9DHCDiIiqDqrqa4QTwXuISDbweeDPLjj6JLavoZPxCV2w/3/KZasLyA74edmWhjDGOCJJAGVA87TXLU7ZrHVUNQT0AoULHPf/AH8FDM1XSUTuFpEaEakJBu3Da8orJzpIT03h8tUFEdVP9aVw9dpCXjkRtBVDjTGAS4PAIrIVWKuq/7pQXVV9TFW3q+r24mJb2XLKqyeDXFFVSHqqL+J9rttYTGvPMKeCg0sYmTEmXkSSAFqBimmvy52yWeuIiB/IA+brbL4K2C4ip4HXgA0i8lJkIZupD/EPrY+s/3/KVHeRdQMZYyCyBLAfWC8iVSKSBuwG9s6osxe409m+DXhB5+lnUNWvqupKVa0EPgicUNXr32/wyeo1Z/rn+13rv2JZJmuLsywBGGOACJaDVtWQiNwLPA/4gK+r6hEReRCoUdW9wOPAN0WkDuginCQAcL7l5wJpInIrcKOqHo3+qSSPNxu6KcoOsG559vve99oNxfzTviZGxifeV/eRMSbxRPQ8AFV9Fnh2RtkD07ZHgNvn2LdygWOfBrZEEocJO9rWx5ay3Ata4/+6DcU88fpp3mzo4lp7WpgxSc3uBI4zY6FJ6s71U12ae0H7X766ABE42NQT5ciMMfHGEkCcOXG2n/EJpXrlhSWAnPRU1i/P5lBzd5QjM8bEG0sAceZoW3hFzwttAQBcWp7PoeYeux/AmCRnCSDOHD3TR2aaj9WFWRd8jK2r8ukeGqe5aziKkRlj4o0lgDhztK2PTSU5+FIu/CHvWyvyATho3UDGJDVLAHFEVTl2pu+C+/+nbFyRQ3pqCoeabSDYmGRmCSCOtHQP0z8aoro0b1HH8ftSuLgsj7ctARiT1CwBxJEjZ5wB4EW2ACDcDXT4TB9joclFH8sYE58sAcSRo219pEi4C2extlYUMBaatOcEG5PELAHEkaNn+lhTnE1G2uKXcLi0ItyNZN1AxiQvSwBx5Fhb36Lm/09Xlp9BUXaAg5YAjElalgDiRM/QGK09w1Hp/wcQEbZW5NtMIGOSmCWAOBGNO4Bn2rYqn/rgIL3D9qB4Y5KRJYA4cTSKM4CmXFoeviHsnRZrBRiTjCwBxImjbX0szwlQlB2I2jEvLg8PBL/b2hu1Yxpj4oclgDhxKjh4QQ+AmU9eRirlBRnnWxfGmORiCSAOqCr1wQHWFF/4AnBzqS7N5VibJQBjkpElgDjQOThG/0iINUXRbQFAeEyhvmOQobFQ1I9tjPG2iBKAiOwUkVoRqROR+2Z5PyAiTznv7xORSqe8UEReFJEBEfnKtPqZIvIfInJcRI6IyF9G64QSUX1wEGDJWgCqUNveH/VjG2O8bcEEICI+4FHgZqAauENEqmdUuwvoVtV1wJeAh53yEeCPgN+d5dBfUNVNwDbgGhG5+cJOIfHVBwcAlqQFsNmZVnrUuoGMSTqRtAB2AHWqWq+qY8AeYNeMOruAJ53tZ4AbRERUdVBVXyOcCM5T1SFVfdHZHgPeAsoXcR4JraFjkDR/CmUFGVE/dnlBBjnpfhsINiYJRZIAyoDmaa9bnLJZ66hqCOgFCiMJQETygZ8BfhBJ/WR0KjhIZWHmoh4CMxcRsYFgY5KUq4PAIuIHvg08oqr1c9S5W0RqRKQmGAzGNkCPqO8YWJLunynVK3M53t7PxKQ9I9iYZBJJAmgFKqa9LnfKZq3jfKjnAZ0RHPsx4KSqfnmuCqr6mKpuV9XtxcXFERwysYxPTNLUOUTVEgwAT9lcmsvQ2ASNnYNL9juMMd4TSQLYD6wXkSoRSQN2A3tn1NkL3Ols3wa8oKrzfp0UkT8jnCh++/2FnFxauocJTSpripYuAVTbQLAxSWnBBOD06d8LPA8cA55W1SMi8qCIfNyp9jhQKCJ1wOeB81NFReQ08EXgF0WkRUSqRaQc+EPCs4reEpFDIvIr0TyxRHF+BlDx0nUBrV+RjT9FbCDYmCTjj6SSqj4LPDuj7IFp2yPA7XPsWznHYaM/opmAzt8DsIQtgIDfx7rl2dYCMCbJ2J3AHlffMUhBZioFWWlL+ntsJpAxyccSgMeF1wBauu6fKdUrcznbN0rHwOiS/y5jjDdYAvC4+o7BJe3+mTI1EGytAGOShyUAD+sfGSfYP7qkU0CnnF8SwgaCjUkalgA8rKFjagB46buACrLSKMlNt0XhjEkilgA8bGoG0NoYtAAANpbkcMwSgDFJwxKAh9UHB0gRWFWYGZPft6k0h7pz/YxPTMbk9xlj3GUJwMPqOwYpL8gk4PfF5PdtLsllfELPdz0ZYxKbJQAPO905SGUMZgBN2ViSA9hMIGOShSUAD2vqHGL1sth0/wCsLQ4vCXHcxgGMSQqWADyqZ2iMvpEQq2PU/w+Q5k9h3fJsmwlkTJKwBOBRjZ1DAKyKYQsAwt1Ax60LyJikYAnAoxq7nAQQwxYAwKaSXM70jtA7NB7T32uMiT1LAB7V3OVOC2BTaXgguPasdQMZk+gsAXhUY+cgxTkBMtMiWrE7ajY5M4GOt1s3kDGJzhKARzV2DsX82z9ASW46eRmpHGuzFoAxic4SgEc1dcV2CugUEWFTSQ611gIwJuFZAvCgkfEJ2vtGYj4APCWcAPqZnJz3sc7GmDhnCcCDWrqHUSWm9wBMt6k0l8GxCVp7hl35/caY2IgoAYjIThGpFZE6EblvlvcDIvKU8/4+Eal0ygtF5EURGRCRr8zY53IRedfZ5xERsWcEO5q6wmvxuDEGAD8eCLYlIYxJbAsmABHxAY8CNwPVwB0iUj2j2l1At6quA74EPOyUjwB/BPzuLIf+KvA/gfXOz84LOYFE9OObwGK3DtB0G1bkIIItCWFMgoukBbADqFPVelUdA/YAu2bU2QU86Ww/A9wgIqKqg6r6GuFEcJ6IlAK5qvqGqirwDeDWxZxIImnqGiIzzUdR9tI+CH4uWQE/q5Zl2lRQYxJcJAmgDGie9rrFKZu1jqqGgF6gcIFjtixwTABE5G4RqRGRmmAwGEG48a/JmQLqZq/Y5pJcmwpqTILz/CCwqj6mqttVdXtxcbHb4cREY5c79wBMt6k0h9OdgwyNhVyNwxizdCJJAK1AxbTX5U7ZrHVExA/kAZ0LHLN8gWMmpclJDd8D4NIMoCmbS3NRxVYGNSaBRZIA9gPrRaRKRNKA3cDeGXX2Anc627cBLzh9+7NS1TagT0SudGb//ALw7+87+gR0rn+UsdAkqwrdGQCesrkkF7CBYGMS2YILzahqSETuBZ4HfMDXVfWIiDwI1KjqXuBx4JsiUgd0EU4SAIjIaSAXSBORW4EbVfUo8OvAPwAZwHPOT9Jr7AxPAXXjLuDpygsyyA74bSqoMQksopXGVPVZ4NkZZQ9M2x4Bbp9j38o5ymuALZEGmiwaXVoFdKaUFHGeDWAtAGMSlecHgZNNU+cQvhShrCDD7VDYXJrDsfY+5unNM8bEMUsAHtPUNcTK/HRSfe7/02wqyaV/JGRLQhiToNz/lDHv4YUpoFM2l4YHgu1+AGMSkyUAj2nuGnJtCYiZNk49HMYGgo1JSJYAPGRgNETX4BgVy9zv/wfIDvhZXZjJMVsSwpiEZAnAQ9x6DvB8NpXkWBeQMQnKEoCHNHkwAWwuzbUlIYxJUJYAPMSbLQBbEsKYRGUJwEOau4bICfjJy0h1O5TzqkttSQhjEpUlAA9p6hqiwuVloGcqL8ggK81nS0IYk4AsAXhIk4fuAZiSkiJUr8zlyBlLAMYkGksAHjE5qTR3D7PK5WWgZ3PRyjyOnuljYtKWhDAmkVgC8IjgQHgZ6AoPrAE005ayPIbHJ2joGHA7FGNMFFkC8IipKaAVHusCAthSFh4Itm4gYxKLJQCPaOr03hTQKWuLs0nzp3C4tdftUIwxUWQJwCOau4cQwRPLQM+U6kthc0kOh1utBWBMIrEE4BFNXUOU5KYT8PvcDmVWF5XlceRMrz0bwJgEYgnAI5qdewC8asvKPPpGQrR027MBjEkUlgA8wov3AEw3NRBs4wDGJI6IEoCI7BSRWhGpE5H7Znk/ICJPOe/vE5HKae/d75TXishN08o/JyJHROSwiHxbRNKjcULxaGR8grN9o1QUeDcBbFiRgy9FOHzGEoAxiWLBBCAiPuBR4GagGrhDRKpnVLsL6FbVdcCXgIedfauB3cBFwE7gb0XEJyJlwG8C21V1C+Bz6iWlqW6VVYXeGwCekp7qY/3ybBsINiaBRNIC2AHUqWq9qo4Be4BdM+rsAp50tp8BbpDwgja7gD2qOqqqDUCdczwAP5AhIn4gEzizuFOJX15cBXQ2W2wg2JiEEkkCKAOap71uccpmraOqIaAXKJxrX1VtBb4ANAFtQK+q/udsv1xE7haRGhGpCQaDEYQbf7x8E9h0W1bm0jEwxrn+UbdDMcZEgSuDwCJSQLh1UAWsBLJE5Odmq6uqj6nqdlXdXlxcHMswY6a5a4j01BSKswNuhzKvLWV5gA0EG5MoIkkArUDFtNflTtmsdZwunTygc559Pwo0qGpQVceB7wBXX8gJJIKmriEqCry1DPRsNpfmIoKNAxiTICJJAPuB9SJSJSJphAdr986osxe409m+DXhBwx3Fe4HdziyhKmA98Cbhrp8rRSTTGSu4ATi2+NOJT16fAjolK+CnqiiLd60FYExC8C9UQVVDInIv8Dzh2TpfV9UjIvIgUKOqe4HHgW+KSB3QhTOjx6n3NHAUCAH3qOoEsE9EngHecsoPAo9F//S8T1Vp6hriyjWFbocSka3l+bxysgNV9XyLxRgzvwUTAICqPgs8O6PsgWnbI8Dtc+z7EPDQLOV/DPzx+wk2EXUMjDE0NsFqDz4HYDbbVuXznYOttPYMU+7h+xaMMQuzO4Fd1tQ1CBA3CWBrRQEAh5p7XI7EGLNYlgBc1nh+GegslyOJzKbSHAL+FA42WQIwJt5ZAnBZY2d4GeiKZd69C3i6VF8KF5flWQvAmARgCcBlTV1DlHp4GejZbK3I53BrL2OhSbdDMcYsgiUAlzV2DnryQfDz2boqn9HQJMfb7X4AY+KZJQCXNXUNsTpO+v+nbFtlA8HGJAJLAC4aGA3RMTAWdy2AlXnpFOcEOGQDwcbENUsALpp6EHy8TAGdIiJsrcjnoLUAjIlrlgBcdP4egDjrAoLwDWENHYP0DI25HYox5gJZAnDR+XsA4qwFAOGZQGDjAMbEM0sALmrsGiI/M5W8jFS3Q3nfLinPRwS7IcyYOGYJwEVNnUOsjoNVQGeTHfCzcUWOjQMYE8csAbiosWuQVYXx1/8/ZXtlAQdOdxGasBvCjIlHlgBcMj4xyZmekbhtAQBcUVXI4NgER87YDWHGxCNLAC5p7R5mYlLjcgB4yhVrlgGwr6HT5UiMMRfCEoBLGp0HwVfGcRfQ8px01hRlsa++y+1QjDEXwBKAS5o64+s5AHO5Ys0y3mzoYmJS3Q7FGPM+WQJwSWPnEOmpKSzPCbgdyqJcUVVI/2iIY202DmBMvLEE4JJG50Hw8f5c3alxgDfqbRzAmHgTUQIQkZ0iUisidSJy3yzvB0TkKef9fSJSOe29+53yWhG5aVp5vog8IyLHReSYiFwVjROKF02dQ3HzFLD5lOZlsLowk30NNg5gTLxZMAGIiA94FLgZqAbuEJHqGdXuArpVdR3wJeBhZ99qYDdwEbAT+FvneAB/DXxfVTcBlwLHFn868WFyUmnsGoz7/v8pV1QtY//pLiZtHMCYuBJJC2AHUKeq9ao6BuwBds2oswt40tl+BrhBwn0bu4A9qjqqqg1AHbBDRPKAa4HHAVR1TFWT5pbStr4RRsYnWVMc/y0ACI8D9AyNU3u23+1QjDHvQyQJoAxonva6xSmbtY6qhoBeoHCefauAIPCEiBwUka+JyKyfhiJyt4jUiEhNMBiMIFzvawiGZwBVFSVIApi6H8DGAYyJK24NAvuBy4Cvquo2YBD4ibEFAFV9TFW3q+r24uLiWMa4ZOo7BgBYW5ztciTRUV6QSXlBBm/Y/QDGxJVIEkArUDHtdblTNmsdEfEDeUDnPPu2AC2qus8pf4ZwQkgK9cFBstJ8cT8FdLqr1hTyo/pOWxfImDgSSQLYD6wXkSoRSSM8qLt3Rp29wJ3O9m3AC6qqTvluZ5ZQFbAeeFNV24FmEdno7HMDcHSR5xI3TgUHWFOcHfdTQKe7bmMxvcPjvN2SNEM5xsQ9/0IVVDUkIvcCzwM+4OuqekREHgRqVHUv4cHcb4pIHdBFOEng1Hua8Id7CLhHVSecQ/8G8C0nqdQDvxTlc/Osho5BLnMerJ4oPrSumBSBl2qDXL56mdvhGGMisGACAFDVZ4FnZ5Q9MG17BLh9jn0fAh6apfwQsP39BJsIRsYnaO0Z5rbLy90OJaryMlO5bFUBL9UG+Z0bNy68gzHGdXYncIyd7hxEFdYkyADwdNdvLObd1l6C/aNuh2KMiYAlgBird6aArkmQKaDTXb9xOQCvnEiM6brGJDpLADHW0JFY9wBMV12aS1F2gJcsARgTFywBxNip4AAluelkBSIafokrKSnCdRuKefVk0JaHNiYOWAKIsfrgYMIsATGb6zcW0zM0ziF7WLwxnmcJIIZUlfrgQEJ2/0z50PoiUgRerj3ndijGmAVYAoihrsEx+kZCCTkDaEp+ZhrbVhXwYq2NAxjjdZYAYqjeGQBO5C4ggBs2L+fd1l5ae4bdDsUYMw9LADFUH3QWgStK3BYAwC1bSgF47t02lyMxxszHEkAM1XcMkuZLoawgw+1QllRlURbVpbn8hyUAYzzNEkAM1QfDTwHzpSTOInBz+elLSjnY1MMZ6wYyxrMsAcRQfXAg4fv/p9xysdMNdLjd5UiMMXOxBBAjoYlJmrqGqErw/v8pVUVZbC7N5VnrBjLGsywBxEhDxyDjE8qGFcmRAABu2VLCgcZu2nqtG8gYL7IEECPH28MPTN9UkutyJLFzyyXhbqDvWzeQMZ5kCSBGjrf34UsR1i5PjjEACD/zeFNJjnUDGeNRlgBipLa9nzVFWQT8PrdDiamfvriU/ae7ae4acjsUY8wMlgBi5Hh7PxtLctwOI+Y+eXk5IvDMgRa3QzHGzGAJIAb6R8Zp6R5mc2ny9P9PKcvP4IPrinjmQAuTtkS0MZ4SUQIQkZ0iUisidSJy3yzvB0TkKef9fSJSOe29+53yWhG5acZ+PhE5KCLfW+yJeNmJs+EB4I0rkq8FAHD79gpae4b5UX2n26EYY6ZZMAGIiA94FLgZqAbuEJHqGdXuArpVdR3wJeBhZ99qYDdwEbAT+FvneFN+Czi22JPwuqkZQMnYBQRwY/UKctP9PF3T7HYoxphpImkB7ADqVLVeVceAPcCuGXV2AU86288AN4iIOOV7VHVUVRuAOud4iEg58NPA1xZ/Gt5W295PdsBPeYKvATSX9FQft24r47nD7fQOjbsdjjHGEUkCKAOmf3VrccpmraOqIaAXKFxg3y8Dvw9MzvfLReRuEakRkZpgMD7XmD/e3s+GFdmEc2Jy+tT2CsZCk+x954zboRhjHK4MAovIx4Bzqnpgobqq+piqblfV7cXFxTGILrpUldr2fjYl4QDwdBetzGVzaS7PWDeQMZ4RSQJoBSqmvS53ymatIyJ+IA/onGffa4CPi8hpwl1KHxGRf7yA+D2vvW+E3uFxNiVp//8UEeHT28t5u6WXd1t63Q7HGENkCWA/sF5EqkQkjfCg7t4ZdfYCdzrbtwEvqKo65budWUJVwHrgTVW9X1XLVbXSOd4LqvpzUTgfzzk/AJykM4Cm++Tl5WSl+Xji9Qa3QzHGEEECcPr07wWeJzxj52lVPSIiD4rIx51qjwOFIlIHfB64z9n3CPA0cBT4PnCPqk5E/zS8qzYJ1wCaS256Krdvr+C775zhXN+I2+EYk/QiGgNQ1WdVdYOqrlXVh5yyB1R1r7M9oqq3q+o6Vd2hqvXT9n3I2W+jqj43y7FfUtWPReuEvKa2vZ/SvHTyMlPdDsUTfvHqSkKTyj/ua3I7FGOSnt0JvMSOtfUl7fz/2VQWZfGRjcv5p32NjIwnVWPQGM+xBLCExicmORUcsAQwwy9dU0XHwBjffdumhBrjJksAS+jE2X7GJ5TqJJ8COtM16wrZsCKbJ14/TXiugDHGDZYAltDBph4ALltV4HIk3iIi/PI1VRxt6+O1ug63wzEmaVkCWEIHm3oozEpL2iUg5vOJy8ooyU3nkR+ctFaAMS6xBLCEDjV3s21VflIvATGXgN/HZ69bw/7T3bxR3+V2OMYkJUsAS6R3aJxTwUG2WffPnHbvWEVxToC/eeGk26EYk5QsASyRQy3h/v+tFfkuR+Jd6ak+fvXaNfzwVCc1p60VYEysWQJYIgebuhGBS8rz3A7F0z5zxSoKs9J45IU6t0MxJulYAlgih5p72LA8h5x0uwN4Pplpfn7lQ2t45USQA43WCjAmliwBLAFV5WBTD9tWWfdPJO68ejXLcwI89B/HbEaQMTFkCWAJNHQM0js8bv3/EcpM8/P5n9rAW009fP9wu9vhGJM0LAEsgakbwGwGUORu317BhhXZPPz944yF5n1InDEmSiwBLIFDzT1kB/ysW57tdihxw5ci3H/zZk53DvFP+xrdDseYpGAJYAkcbO7m0oo8fCl2A9j7cf3GYq5eW8hf/+AkvcP28HhjlpolgCgbHpvgWFu/9f9fABHhf92ymd7hcb74n7Vuh2NMwrMEEGUHGruZmFQuX239/xdiS1kev3BVJd94o5FDzT1uh2NMQrMEEGUvnzhHmi+FK9cUuh1K3PqdGzewIied+7/zLqEJGxA2ZqlYAoiyl08E2VG1jMw0v9uhxK2c9FT+5OMXcaytjydeP+12OMYkrIgSgIjsFJFaEakTkftmeT8gIk857+8Tkcpp793vlNeKyE1OWYWIvCgiR0XkiIj8VrROyE1neoY5cXaA6zYUux1K3LvpohV8dPMKvvhfJ2juGnI7HGMS0oIJQER8wKPAzUA1cIeIVM+odhfQrarrgC8BDzv7VgO7gYuAncDfOscLAb+jqtXAlcA9sxwz7rx8IgiEZ7OYxRER/nTXRfhShM89dci6goxZApG0AHYAdapar6pjwB5g14w6u4Anne1ngBskvAj+LmCPqo6qagNQB+xQ1TZVfQtAVfuBY0DZ4k/HXS/XBlmZl27z/6OkLD+DP7t1CzWN3fyNLRZnTNRFkgDKgOZpr1v4yQ/r83VUNQT0AoWR7Ot0F20D9s32y0XkbhGpEZGaYDAYQbjuGJ+Y5PW6Dq7bWGwPgImiW7eV8cltZfzNCyd5s8EWizMmmlwdBBaRbOBfgN9W1b7Z6qjqY6q6XVW3Fxd7t2vlrcZu+kdDXLdhuduhJJwHb91CxbJMfnvPQXqH7AYxY6IlkgTQClRMe13ulM1aR0T8QB7QOd++IpJK+MP/W6r6nQsJ3ktePhHEnyJcvc6mf0ZbdsDPI7u3ca5/lHu//RbjNh5gTFREkgD2A+tFpEpE0ggP6u6dUWcvcKezfRvwgobX9d0L7HZmCVUB64E3nfGBx4FjqvrFaJyI216qDXLZ6gJybf3/JXFpRT5//omLefVkBw/8+xFbNtqYKFgwATh9+vcCzxMerH1aVY+IyIMi8nGn2uNAoYjUAZ8H7nP2PQI8DRwFvg/co6oTwDXAzwMfEZFDzs8tUT63mDnXN8LRtj6b/bPEPvWBCn7t+rV8+80m/v7VerfDMSbuRXS3kqo+Czw7o+yBadsjwO1z7PsQ8NCMsteAhBkp/d47bQDcsGmFy5Ekvt+7cSNNnUP8xXPHWZGbzq6tcT95zBjX2O2qi6SqPF3TzKXleWwsyXE7nISXkiL81acuJTgwyueeOsTo+CSf+kDFwjsaY36CLQWxSO+09HK8vZ/bt4Hq6WgAAA26SURBVNuHUKykp/p48pd2cM26In7/X97hidcb3A7JmLhkCWCRnq5pJj01hY9vXel2KEklI83H1+7czk0XreBPv3uUv3jumM0OMuZ9sgSwCMNjE+w9dIZbtpTa7B8XBPw+Hv3MZXzmilX8v5fr2f3YG7R027pBxkTKEsAiPHe4jf7RkPVBu8jvS+HPP3Exj9yxjdr2fm7561f5lwMtTE7aNFFjFmKDwIvw1P5mKgszuaJqmduhJL2PX7qSS8vz+K09h/idf36br73WwB/s3Mh1G2K3NMfQWIimriEaO4do7hqic3CM3uFx+obHUSDgSyHNn0JRdoBVyzKpWJbJppIcCrLSYhKfMTNZArhApzsG2dfQxe/dtNHW/vGI1YVZfOfXrua775zhC/9Zyy8+sZ9LK/K57fJyfuaSUvIzo/NBOzmpNHcPcfRMH8fa+jje3s/x9n6aZixb7UsR8jNSyc1IRQTGQpOMhibpGhxjYloLZeOKHHZULePaDcVcu6GIgN8XlTiNWYjE0x2V27dv15qaGrfDAOBzTx3i2XfbePX3P8zy3HS3wzEzjIUm2bO/iW+90UTt2X5SfcJVa4u4fFUB21bls7k0l8KsNFJS5k7eg6MhmruHaO4aprFzkBNn+zlxdoCTZ/sZHJsAIEWgsiiLTSU5bFyRS1VxFquXZbJqWSb5mamzfjkYn5ikrWeE052DvNPSw76GLg40djM0NkFOup+bLirh1q1lXLOu0L5cmKgQkQOquv0nyi0BvH/vtvTyM195jV+/fi2/v3OT2+GYeagqR9v6+M5brbx2soMT5/qZ+pP3pwjLcwLkZ6YhAiIQmlB6h8fpHhpjZPy9s4oKs9LYsCKHDSuy2VyaS/XKXDasyCE9dfHf2McnJvnhqU6++/YZnj/cTv9oiDXFWfz8lav52cvLbZKBWRRLAFGiqtzx929w8uwAL/3e9eTY/5hxpW9knLebezh1boBz/aOc7Ruld3gMVVB+3G2Tn5nKsqwA5QUZVDjf6JfFqK9+ZHyC5w638eQPGznU3EN2wM//uGIVv/zBKlZYa9NcAEsAUfKDY2e568kaHtx1Eb9wVaWrsZjE905LD197tYHvvXMGf0oKP3t5Gb923TpWFWa6HZqJI5YAoiA0MclNX34FVXj+c9eS6rNZtCY2GjsHeeyVev65poUJVW7dWsY9H17LmmJ7+pxZ2FwJwD7B3oevvnSKU8FB7rt5k334m5haXZjFQ5+4mFf/4MPceVUl33vnDB/94sv85rcPUtve73Z4Jk5ZCyBC3z/czmf/8QC7tq7ky5/earMzjKuC/aM8/loD3/zRaQbHJrixegWfvX4tl60qcDs040HWBbQIR870cttXf8TGkhz23H1lVGZ9GBMNPUNjPPH6af7hh6fpHR7nA5UF3H3tWj6yaTm+eaa4muRiCeACne0b4ROPvo4C/37vNSzPsVkYxnsGR0M8tb+Zx19roLVnmIplGfzcFav51PYKu9PYWAK4EPvqO/mNbx9kYDTE0796FVvK8mL2u425EKGJSb5/pJ1v/KiRNxu6CPhTuPGiEj55WRkfWleE38auktJcCcCWgpjF5KTyd6+c4gvP11JZmMU37trBppJct8MyZkF+Xwofu2QlH7tkJcfb+/infU3sffsM3337DEXZAXZuWcGN1SVcuaaQNH9yJQNVZXh8gsHRCYbGQoyGwjf6CeEHDWUH/GQF/GSm+ua9QzyRWAtgmvGJSZ59t43HXqnnyJk+PnZJKX/5s5eQHbA8aeLXWGiSF2vP8W8HW3mpNsjweHjJiavXFnL12iKuXlvIuuXZcT+xYXD0x4vxtXQP0dI9TGvPMGf7RujoH6VjcIyx0MLPjPClCIVZaRTnBFieE2BlfgblBZmUFWSwelkmqwszo7auVKwsqgtIRHYCfw34gK+p6l/OeD8AfAO4HOgEPq2qp5337gfuAiaA31TV5yM55myWIgGMhiY42NTD63UdfOetVlp7hllbnMU9H17HJ7aVxf3/FMZMNzI+wWsnO/jvY2d5/VQHzV3DAOSm+7m4PI+Ly/LZXJrD2uJsqoqyyPLQl5/QxCTBgVFauodp7nLWaOoapKlziNOdQ3QMjL6nflaaj7KCDEryMijODlCUnUZBVtr5b/mB1JRpx1YGx0IMjIToGxmnc2CMYP8oZ/tHaO0epnto/D3HzstIZXVhJqsLw2s/VSzLoKIgk/KCTEry0j3XurrgBCAiPuAE8FNAC7AfuENVj06r8+vAJar6WRHZDXxCVT8tItXAt4EdwErgv4ENzm7zHnM2F5oAmrvCfxw9Q+E1Xs70DNPQMcTpzkGOnuljeHyCFIEdVcv4lQ+u4SOblidNE9Akt+auIX5U38mh5h7ebenleHsf4xM//kwoyg6wMj+d0rx0VuSmU5CZxrKsNPIzU8lKC3eZZAV8BPw+0lNTSPWl4E8RfClCighT359UYUKVyUklNKmMhSYZm5hkdHySobEQQ+MTDI1O0DcyTu9w+KdrYIzOwVGCA2Oc7R3hXP8IMx/zUJKbzurCTCoLs1hVGP52vnpZFhXLMsjLmH0xvgsxMBqipXuIps5wC+N05+D51kZrz/B7VncNX7c0VuSmU5wToDArnHzyM9PIzfCTl5F6PgllBfykp6YQ8PtI86eQ5kvB7xNSfSn4UgSfcw0Xex6LGQPYAdSpar1zoD3ALmD6h/Uu4E+c7WeAr0g44l3AHlUdBRpEpM45HhEcM2rufOJN6oOD7ylbkRugsjCLT3+ggqvXFnLFmkLyMmxdH5NcKpznEnzKeab1aGiCxs4hTp0b4FRw4Hw3yqngIG/Ud9E7PL7AEaPDnyIsy0qj0Pnmvn55EaV56ZTkpVOWH16fqSw/I2ZTsrMDfjaV5M46Fjg+MUl77wjNXeFup7beEdr7wv/tGBjlRHt/xN1Pc0kROPrgzqifbyQJoAxonva6BbhirjqqGhKRXqDQKX9jxr5lzvZCxwRARO4G7nZeDohIbQQxz6YI6Jh60Qi8eYEHWkLvidGDvB4fWIzR4In4Ts3/tidiXEBUY8xYsJN8XqtnK/ROB98cVPUx4LHFHkdEamZrAnmJ12P0enxgMUaD1+MDizFaIhmpaAWmP/S23CmbtY6I+IE8woPBc+0byTGNMcYsoUgSwH5gvYhUiUgasBvYO6POXuBOZ/s24AUNjy7vBXaLSEBEqoD1hHteIjmmMcaYJbRgF5DTp38v8DzhKZtfV9UjIvIgUKOqe4HHgW86g7xdhD/Qceo9TXhwNwTco6oTALMdM/qn9x6L7kaKAa/H6PX4wGKMBq/HBxZjVMTVjWDGGGOix1t3KxhjjIkZSwDGGJOkEj4BiMifiEiriBxyfm6Z9t79IlInIrUicpOLMe50YqgTkfvcimMmETktIu86163GKVsmIv8lIied/8b0CSQi8nUROScih6eVzRqThD3iXNd3ROQyl+Lz1N+giFSIyIsiclREjojIbznlnriO88TnmesoIuki8qaIvO3E+KdOeZWI7HNiecqZ5IIzEeYpp3yfiFQudYwRUdWE/iF8h/LvzlJeDbwNBIAqwved+FyIz+f87jVAmhNTtdvXzYntNFA0o+z/Avc52/cBD8c4pmuBy4DDC8UE3AI8R3jBxyuBfS7F56m/QaAUuMzZziG8LEu1V67jPPF55jo61yLb2U4F9jnX5mlgt1P+d8CvOdu/Dvyds70beGqp/50j+Un4FsA8zi9ToaoNwPRlKmLp/FIbqjoGTC2L4VW7gCed7SeBW2P5y1X1FcIzzSKJaRfwDQ17A8gXkVIX4puLK3+Dqtqmqm852/3AMcJ36HviOs4T31xifh2dazHgvEx1fhT4COHlcOAnr+HUtX0GuEHE/ZUmkyUB3Os0Xb8+rctitiUu5vsjWypeiWM2CvyniByQ8JIcACtUtc3ZbgdWuBPae8wVk5eurSf/Bp2uiG2Ev8F67jrOiA88dB1FxCcih4BzwH8Rbnn0qGpoljjes1wOMLVcjqsSIgGIyH+LyOFZfnYBXwXWAluBNuCvXA02vnxQVS8DbgbuEZFrp7+p4fasp+YRezEmPPo3KCLZwL8Av62qfdPf88J1nCU+T11HVZ1Q1a2EVzLYAWxyM54L4fm1gCKhqh+NpJ6I/D3wPeelV5aj8EocP0FVW53/nhORfyX8R35WREpVtc3pBjjnapBhc8XkiWurqmentr3yNygiqYQ/XL+lqt9xij1zHWeLz4vX0YmrR0ReBK4i3D3md77lT49jKsYWee9yOa5KiBbAfGb0VX4CmJqdMdcyFbHmyWUxRCRLRHKmtoEbCV+76ct+3An8uzsRvsdcMe0FfsGZxXIl0DutiyNmvPY36PQ9Pw4cU9UvTnvLE9dxrvi8dB1FpFhE8p3tDMLPNjkGvEh4ORz4yWs423I57nJ7FHqpf4BvAu8C7xD+Ryid9t4fEu63qwVudjHGWwjPdDgF/KHb18yJaQ3hmRVvA0em4iLcb/kD4CThB/wsi3Fc3ybc/B8n3Md611wxEZ6p8ahzXd8FtrsUn6f+BoEPEu7eeQc45Pzc4pXrOE98nrmOwCXAQSeWw8ADTvkawsmnDvhnIOCUpzuv65z318Ti33qhH1sKwhhjklTCdwEZY4yZnSUAY4xJUpYAjDEmSVkCMMaYJGUJwBhjkpQlAGOMSVKWAIwxJkn9fxrktV38hnClAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.kdeplot([len(doc) for doc in test_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words in the train dataset:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('scsi', 37),\n",
       " ('orbit', 33),\n",
       " ('space', 29),\n",
       " ('insur', 28),\n",
       " ('jew', 27),\n",
       " ('imag', 26),\n",
       " ('window', 26),\n",
       " ('control', 24),\n",
       " ('launch', 23),\n",
       " ('list', 23),\n",
       " ('armi', 23),\n",
       " ('power', 22),\n",
       " ('engin', 21),\n",
       " ('drive', 21),\n",
       " ('speed', 20),\n",
       " ('understand', 20),\n",
       " ('option', 20),\n",
       " ('hand', 19),\n",
       " ('current', 19),\n",
       " ('close', 19)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Most common words in the train dataset:\")\n",
    "Counter([word for doc in train_texts for word in doc]).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words in the test dataset:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('muslim', 31),\n",
       " ('game', 27),\n",
       " ('govern', 25),\n",
       " ('april', 25),\n",
       " ('nation', 24),\n",
       " ('drive', 23),\n",
       " ('perform', 21),\n",
       " ('player', 21),\n",
       " ('group', 20),\n",
       " ('command', 20),\n",
       " ('happen', 19),\n",
       " ('test', 19),\n",
       " ('card', 19),\n",
       " ('degre', 19),\n",
       " ('play', 18),\n",
       " ('send', 17),\n",
       " ('design', 17),\n",
       " ('call', 15),\n",
       " ('feel', 15),\n",
       " ('andrew', 15)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Most common words in the test dataset:\")\n",
    "Counter([word for doc in test_texts for word in doc]).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num documents: 100\n",
      "Num unique words: 1000\n"
     ]
    }
   ],
   "source": [
    "print(\"Num documents: {}\".format(dictionary.num_docs))\n",
    "print(\"Num unique words: {}\".format(len(dictionary.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LDA(BaseLDA):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_optim_steps: int,\n",
    "        num_topics: int,\n",
    "        dictionary: gensim.corpora.Dictionary,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            num_optim_steps=num_optim_steps,  # number of optimization step\n",
    "            num_topics=num_topics,  # number of topics\n",
    "            dictionary=dictionary,  # contains the information about mappings word to indices\n",
    "        )\n",
    "    \n",
    "    def assign_topics_to_words(\n",
    "        self,\n",
    "        data: List[torch.Tensor],\n",
    "        document_topic_dist_theta: torch.Tensor,\n",
    "        topic_word_dist_phi: torch.Tensor,\n",
    "    ) -> List[torch.Tensor]:\n",
    "        \"\"\"Generate topic assigments for each word in each document.\n",
    "        \n",
    "        :param dataset: Each entry is a list of indices (a single document)\n",
    "            with the variable length and the whole parameter represents list of \n",
    "            documents. In a single document, words are indices from the dictionary\n",
    "            (tensor N_d of longs).\n",
    "        :param document_topic_dist_theta: Matrix D x K where each row entry\n",
    "            has parameters of the topic distributions (a'ka probability that\n",
    "            the document has a particular topic)\n",
    "        :param topic_word_dist_phi: Matrix K x V where each row entry\n",
    "            has parameters of the topic distributions (a'ka probability that\n",
    "            the topic has a particular word).\n",
    "        :return: List of topic assignments for each word in each document\n",
    "            sampled from the categorical distribution.\n",
    "        \"\"\"\n",
    "        # TODO: Sample topic assignments for each word in each document\n",
    "\n",
    "        assignmets = []\n",
    "\n",
    "        for d, word_indices in enumerate(data):\n",
    "            z_d = []\n",
    "            for v in word_indices:\n",
    "                p = ((document_topic_dist_theta[d,:]).log() +\n",
    "                    (topic_word_dist_phi[:,v]).log()).exp()\n",
    "                z_di = dist.Categorical(p).sample()\n",
    "                z_d.append(z_di)\n",
    "            assignmets.append(z_d)\n",
    "        return assignmets\n",
    "        \n",
    "    def estimate_document_topic_distribution_theta(\n",
    "        self, data: List[torch.Tensor], topic_assignments_z: List[torch.Tensor]\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Estimate matrix theta.\n",
    "        \n",
    "        Each entry in the matrix describes probability of the topic\n",
    "        being assigned to the $d$-th document.\n",
    "        :param data: Each entry is a list of indices (a single document)\n",
    "            with the variable length and the whole parameter represents list of \n",
    "            documents. In a single document, words are indices from the dictionary\n",
    "            (tensor N_d of longs).\n",
    "        :param topic_assignments_z: Each entry corresponds to the `data` and \n",
    "            consists of topic assignments for each word in documents.\n",
    "        :return: Estimated matrix theta.\n",
    "        \"\"\"\n",
    "        # TODO: Perform calculation to estimate theta matrix parameters\n",
    "\n",
    "        z = topic_assignments_z\n",
    "        alpha = self.alpha\n",
    "        \n",
    "        topic_indices = torch.arange(self.num_topics)\n",
    "        theta = torch.zeros((len(data), self.num_topics))\n",
    "\n",
    "        for d, word_indices in enumerate(data):\n",
    "            z[d] = torch.tensor(z[d])\n",
    "            for k in topic_indices:\n",
    "                theta[d][k] = alpha + (z[d]==k).sum()\n",
    "                theta[d][k] /= self.num_topics * alpha + len(word_indices)\n",
    "\n",
    "        return theta\n",
    "    \n",
    "    def estimate_topic_word_distribution_phi(\n",
    "        self, data: List[torch.Tensor], topic_assignments_z: List[torch.Tensor],\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Estimate matrix phi.\n",
    "        \n",
    "        Each entry in the matrix describes probability of the word being\n",
    "        chosen for the $k$-th topic.\n",
    "        :param data: Each entry is a list of indices (a single document)\n",
    "            with the variable length and the whole parameter represents list of \n",
    "            documents. In a single document, words are indices from the dictionary\n",
    "            (tensor N_d of longs).\n",
    "        :param topic_assignments_z: Each entry corresponds to the `data` and \n",
    "            consists of topic assignments for each word in documents.\n",
    "        :return: Estimated matrix phi.\n",
    "        \"\"\"\n",
    "         # TODO: Perform calculation to estimate phi matrix paramaters\n",
    "\n",
    "        beta = self.beta\n",
    "        topic_indices = torch.arange(self.num_topics)\n",
    "        phi = torch.zeros(self.num_topics, self.vocabulary_size)\n",
    "        for k in topic_indices:\n",
    "            for v in range(self.vocabulary_size):\n",
    "                nominator = beta\n",
    "                denominator = self.vocabulary_size * beta\n",
    "                for d, word_indices in enumerate(data):\n",
    "                    nominator += (topic_assignments_z[d][word_indices == v] == k).sum()\n",
    "                    denominator += (topic_assignments_z[d] == k).sum()\n",
    "                phi[k][v] = nominator / denominator\n",
    "\n",
    "        return phi\n",
    "\n",
    "        \n",
    "    def run_gibbs_step(self, dataset: List[List[str]]):\n",
    "        \"\"\"Perform single estimation calculation of parameters.\n",
    "        \n",
    "        Estimated parameters should be: counts of topic usage,\n",
    "        counts of a word assigned to a particular topic,\n",
    "        counts of topics assigned in a document.\n",
    "        \n",
    "        :param dataset: Unprocessed list of documents, each document\n",
    "            containing list of words.\n",
    "        \"\"\"\n",
    "        \n",
    "        # all parameters are already initialized in the source code\n",
    "        docs_with_indices = self.get_word_indices_from_strings(dataset)\n",
    "        z = self.topic_assignments\n",
    "        theta = self.document_topic_distribution\n",
    "        phi = self.word_topics_distribution\n",
    "        \n",
    "        self.topic_assignments = self.assign_topics_to_words(docs_with_indices,theta,phi) # TODO\n",
    "        self.document_topic_distribution = self.estimate_document_topic_distribution_theta(docs_with_indices,self.topic_assignments) # TODO\n",
    "        self.word_topics_distribution = self.estimate_topic_word_distribution_phi(docs_with_indices,self.topic_assignments) # TODO\n",
    "        \n",
    "        \n",
    "    def get_perplexity(self, dataset: List[List[str]], iterations: int) -> float:\n",
    "        \"\"\"Calculate perplexity for the provided dataset.\n",
    "\n",
    "        :param dataset: Each entry is a lista of strings (a single document)\n",
    "            with the variable length and the whole parameter represents list of \n",
    "            documents. In a single document, words are strings. \n",
    "        :param iterations: Perform Gibbs sampling for the theta and \n",
    "            topic assignments parameters. It is necessary only if theta \n",
    "            `dataset` is different than the one used during `find_params`\n",
    "            call. Set to -1, to ensure that used dataset is the same as\n",
    "            during test.\n",
    "        :return: A single scalar of perplexity.\n",
    "        \"\"\"\n",
    "        docs_with_indices = self.get_word_indices_from_strings(dataset)\n",
    "        phi = self.word_topics_distribution\n",
    "        if iterations <= 0:\n",
    "            # TODO: Handle case where we evaluate perplexity for the train dataset\n",
    "            pass\n",
    "        else:\n",
    "            # Phi should stay fixed, since we estimated it during `find_params` method\n",
    "            # Need to reestimate theta and z\n",
    "            ... # TODO: Handle case where we evaluate perplexity for any dataset\n",
    "            for _ in range(iterations):\n",
    "                self.topic_assignments = self.assign_topics_to_words(docs_with_indices, self.document_topic_distribution, phi)\n",
    "                self.document_topic_distribution = self.estimate_document_topic_distribution_theta(docs_with_indices, self.topic_assignments)\n",
    "\n",
    "\n",
    "        # TODO: Calculate perplexity for estimated phi and theta.\n",
    "        theta = self.document_topic_distribution\n",
    "        nominator_term = 0\n",
    "        denominator_term = 0\n",
    "        for d, word_indices in enumerate(docs_with_indices):\n",
    "            log_prob = 0\n",
    "            for v in range(self.vocabulary_size):\n",
    "                n_dv = (word_indices == v).sum()\n",
    "                s = (phi[:, v] * theta[d, :]).sum()\n",
    "                log_prob += n_dv * s.log()\n",
    "            nominator_term += log_prob\n",
    "            denominator_term += len(word_indices)\n",
    "\n",
    "        perplexity = torch.exp(- nominator_term / denominator_term )\n",
    "        return perplexity\n",
    "        \n",
    "    def get_word_probas_over_topics_for_doc(self, doc: List[str]) -> np.ndarray:\n",
    "        \"\"\"Get probability of each word belonging to `self.num_topics` topics.\n",
    "        \n",
    "        Make a use of the MAP estimation of the word distribution over topics.\n",
    "        :param doc: Single document consisting of strings (words).\n",
    "        :return: ndarray matrix of dimensions `self.num_topics` x `len(doc)`\n",
    "            of probabilities for each word.\n",
    "        \"\"\"\n",
    "         # TODO: Extract necessary information from estimated matrices\n",
    "\n",
    "        doc_word_indices = self.get_word_indices_from_strings([doc])[0]\n",
    "        \n",
    "        topics_distribution_of_words = np.zeros((self.num_topics, len(doc)))\n",
    "        for i, word_index in enumerate(doc_word_indices):\n",
    "            topics_distribution_of_words[:, i] = self.word_topics_distribution[:, word_index]\n",
    "        \n",
    "        return topics_distribution_of_words\n",
    "        \n",
    "        \n",
    "    def get_topic_for_the_document(self, doc: List[str]) -> int:\n",
    "        \"\"\"Get argmax topic for a document.\n",
    "        \n",
    "        Make a use of the MAP estimation of the topic distribution for each document.\n",
    "        :param doc: Single document consisting of strings (words).\n",
    "        :return: Single int scalar corresponding to a document index.\n",
    "        \"\"\"\n",
    "        # TODO: Extract necessary information from the estimated matrices\n",
    "\n",
    "        doc_index = self.document2index(doc)\n",
    "        document_topic = torch.argmax(self.document_topic_distribution[doc_index])\n",
    "        return document_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b3a5a6d539549209855e0a1b3c01856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Optim step', max=10.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LDA(10, 20, dictionary)\n",
    "history = model.find_params(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train perplexity: {:.4f}\".format(model.get_perplexity(train_texts, -1)))\n",
    "print(\"Test perplexity: {:.4f}\".format(model.get_perplexity(test_texts, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_chart(model, train_texts, start=10, end=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}